{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcaa34e2",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rahulbhoyar1995/NER-Case-Study/blob/main/ner_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_NpwiJEjdSNs",
   "metadata": {
    "id": "_NpwiJEjdSNs"
   },
   "source": [
    "### Author : Rahul Bhoyar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M4GdLy2F9OeH",
   "metadata": {
    "id": "M4GdLy2F9OeH"
   },
   "source": [
    "### Named Entity Recognition (NER)\n",
    "\n",
    "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP) that involves identifying and classifying named entities in text into predefined categories like \"Person\" (PER), \"Location\" (GEO), \"Organization\" (ORG), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43e8e42b-3760-47d4-9d14-6afa15f1247d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chardet\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: chardet\n",
      "Successfully installed chardet-5.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install chardet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CXc0vpcdnxpV",
   "metadata": {
    "id": "CXc0vpcdnxpV"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "VETyiqtqn1FD",
   "metadata": {
    "id": "VETyiqtqn1FD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected encoding: Windows-1252\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "# Detect the encoding of the file\n",
    "with open('ner_dataset.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    encoding = result['encoding']\n",
    "\n",
    "print(f\"Detected encoding: {encoding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "Jj6Iknjrn1Ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Jj6Iknjrn1Ca",
    "outputId": "cdfaee02-bc78-491e-c025-6fe57eb3b960"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723383</th>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723384</th>\n",
       "      <td>Sentence: 33049</td>\n",
       "      <td>He</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723385</th>\n",
       "      <td>NaN</td>\n",
       "      <td>is</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723386</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723387</th>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>723388 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Sentence #           Word  POS  Tag\n",
       "0           Sentence: 1      Thousands  NNS    O\n",
       "1                   NaN             of   IN    O\n",
       "2                   NaN  demonstrators  NNS    O\n",
       "3                   NaN           have  VBP    O\n",
       "4                   NaN        marched  VBN    O\n",
       "...                 ...            ...  ...  ...\n",
       "723383              NaN              .    .    O\n",
       "723384  Sentence: 33049             He  PRP    O\n",
       "723385              NaN             is  VBZ    O\n",
       "723386              NaN            the   DT    O\n",
       "723387              NaN             16  NaN  NaN\n",
       "\n",
       "[723388 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"ner_dataset.csv\", encoding=encoding)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dt0QuguTn8Ry",
   "metadata": {
    "id": "dt0QuguTn8Ry"
   },
   "source": [
    "There are 723388 records divided in 4 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yiA7f_9PoGb9",
   "metadata": {
    "id": "yiA7f_9PoGb9"
   },
   "source": [
    "As a part oof our problem statement we want only two columns : \"Word\" and \"Tag\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "gQWFB8ssn0_g",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "gQWFB8ssn0_g",
    "outputId": "16d74adc-e7c9-4d09-9fa8-45c9ee76f917"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723383</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723384</th>\n",
       "      <td>He</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723385</th>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723386</th>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723387</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>723388 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Word  Tag\n",
       "0           Thousands    O\n",
       "1                  of    O\n",
       "2       demonstrators    O\n",
       "3                have    O\n",
       "4             marched    O\n",
       "...               ...  ...\n",
       "723383              .    O\n",
       "723384             He    O\n",
       "723385             is    O\n",
       "723386            the    O\n",
       "723387             16  NaN\n",
       "\n",
       "[723388 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[[\"Word\",\"Tag\"]]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fwsgv6C2octB",
   "metadata": {
    "id": "Fwsgv6C2octB"
   },
   "source": [
    "Let's see how many null values are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "BWZ8pUeforOm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BWZ8pUeforOm",
    "outputId": "22e7d22e-4110-441b-d376-0fe112a94130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word    5\n",
      "Tag     1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values_count = data.isnull().sum()\n",
    "print(missing_values_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qAV_n_uWovDS",
   "metadata": {
    "id": "qAV_n_uWovDS"
   },
   "source": [
    "Here there are 10 records in Word column with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fSd5RygNn083",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fSd5RygNn083",
    "outputId": "6ab810ff-c3ee-4ef1-98ed-366e89cce769"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with null values in 'Word' or 'Tag' columns:\n",
      "       Word  Tag\n",
      "197658  NaN    O\n",
      "256026  NaN    O\n",
      "257069  NaN    O\n",
      "571211  NaN    O\n",
      "613777  NaN    O\n",
      "723387   16  NaN\n"
     ]
    }
   ],
   "source": [
    "null_values_df = data[data['Word'].isnull() | data['Tag'].isnull()]\n",
    "\n",
    "# Display the rows with null values in 'Word' or 'Tag' columns\n",
    "print(\"Rows with null values in 'Word' or 'Tag' columns:\")\n",
    "print(null_values_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mnkNCl_Qo04D",
   "metadata": {
    "id": "mnkNCl_Qo04D"
   },
   "source": [
    "Removing the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "Tl8X8tAen06d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "Tl8X8tAen06d",
    "outputId": "7852e5ef-c5e4-4e3b-d292-0fd57688603b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after removing rows with null values in 'Word' or 'Tag' columns:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723382</th>\n",
       "      <td>attack</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723383</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723384</th>\n",
       "      <td>He</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723385</th>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723386</th>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>723382 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Word Tag\n",
       "0           Thousands   O\n",
       "1                  of   O\n",
       "2       demonstrators   O\n",
       "3                have   O\n",
       "4             marched   O\n",
       "...               ...  ..\n",
       "723382         attack   O\n",
       "723383              .   O\n",
       "723384             He   O\n",
       "723385             is   O\n",
       "723386            the   O\n",
       "\n",
       "[723382 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.dropna(subset=['Word', 'Tag'])\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(\"\\nDataFrame after removing rows with null values in 'Word' or 'Tag' columns:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3PQ_xBd7pE4s",
   "metadata": {
    "id": "3PQ_xBd7pE4s"
   },
   "source": [
    "Checking the uniques tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "kW6Q0lvPn031",
   "metadata": {
    "id": "kW6Q0lvPn031"
   },
   "outputs": [],
   "source": [
    "unique_tags = list(df[\"Tag\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "UtTKYd2epX5C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UtTKYd2epX5C",
    "outputId": "704d7645-c18b-4e78-b8db-140143aa47d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tags are : ['O', 'B-geo', 'B-gpe', 'B-per', 'I-geo', 'B-org', 'I-org', 'B-tim', 'B-art', 'I-art', 'I-per', 'I-gpe', 'I-tim', 'B-nat', 'B-eve', 'I-eve', 'I-nat']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique tags are :\", unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dGWGQr92pbIx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGWGQr92pbIx",
    "outputId": "2f79450b-2ba5-4adf-9476-bea5bbc43a76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique tags are : 17\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of unique tags are :\", len(unique_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aEwDU4wpi8Y",
   "metadata": {
    "id": "0aEwDU4wpi8Y"
   },
   "source": [
    "Checking unique number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "jH757hU9pl83",
   "metadata": {
    "id": "jH757hU9pl83"
   },
   "outputs": [],
   "source": [
    "unique_words = list(df[\"Word\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "myAcLpg8ptBv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "myAcLpg8ptBv",
    "outputId": "9a9ad0d6-71d0-42ac-925c-ce400e17030c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words are : 29650\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of unique words are :\", len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g5AJRyZep3ps",
   "metadata": {
    "id": "g5AJRyZep3ps"
   },
   "source": [
    "Final Dataframe for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "QurOcozkpl6p",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QurOcozkpl6p",
    "outputId": "4d5b8f51-09bb-4f3d-f964-86dae8691a5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(723382, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eHKQKP8zp9lF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "eHKQKP8zp9lF",
    "outputId": "d2ce45a5-0e3e-43e1-c795-52eaa5e880e0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word Tag\n",
       "0      Thousands   O\n",
       "1             of   O\n",
       "2  demonstrators   O\n",
       "3           have   O\n",
       "4        marched   O"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "Nico7BZ9qKIG",
   "metadata": {
    "id": "Nico7BZ9qKIG"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gv3_NvB0qBnO",
   "metadata": {
    "id": "gv3_NvB0qBnO"
   },
   "source": [
    "### Approach 1: Traditional Machine Learning Algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kAhWZK-gqVIL",
   "metadata": {
    "id": "kAhWZK-gqVIL"
   },
   "source": [
    "It is classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O3aRTPqHrsTc",
   "metadata": {
    "id": "O3aRTPqHrsTc"
   },
   "source": [
    "Step 1: Dividing the dataset into training, testing and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "F_mpPBaXpl38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F_mpPBaXpl38",
    "outputId": "7d9fd2f8-41a0-4002-ee45-0972e1df2492"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data.csv')  # Assuming the dataset is in CSV format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e9c7bf-e4c8-41e0-976b-8ca2ca54f48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723377</th>\n",
       "      <td>attack</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723378</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723379</th>\n",
       "      <td>He</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723380</th>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723381</th>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>723382 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Word Tag\n",
       "0           Thousands   O\n",
       "1                  of   O\n",
       "2       demonstrators   O\n",
       "3                have   O\n",
       "4             marched   O\n",
       "...               ...  ..\n",
       "723377         attack   O\n",
       "723378              .   O\n",
       "723379             He   O\n",
       "723380             is   O\n",
       "723381            the   O\n",
       "\n",
       "[723382 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[[\"Word\",\"Tag\"]]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d21d36c2-81a9-46c0-9e38-4e26acfe1fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words are : 29650\n"
     ]
    }
   ],
   "source": [
    "unique_words = data[\"Word\"].unique()\n",
    "\n",
    "\n",
    "print(f\"Total number of unique words are :\", len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac43b57-2654-408a-80d2-e5f22f9bbe09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after removing duplicates:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723222</th>\n",
       "      <td>Hajj</td>\n",
       "      <td>B-org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723223</th>\n",
       "      <td>Ismail</td>\n",
       "      <td>I-org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723224</th>\n",
       "      <td>Jabber</td>\n",
       "      <td>I-org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723354</th>\n",
       "      <td>Junaid</td>\n",
       "      <td>B-per</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723361</th>\n",
       "      <td>Belgaum</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36519 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Word    Tag\n",
       "0           Thousands      O\n",
       "1                  of      O\n",
       "2       demonstrators      O\n",
       "3                have      O\n",
       "4             marched      O\n",
       "...               ...    ...\n",
       "723222           Hajj  B-org\n",
       "723223         Ismail  I-org\n",
       "723224         Jabber  I-org\n",
       "723354         Junaid  B-per\n",
       "723361        Belgaum  B-geo\n",
       "\n",
       "[36519 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop_duplicates(subset=['Word', 'Tag'])\n",
    "\n",
    "# Print the unique DataFrame\n",
    "print(\"DataFrame after removing duplicates:\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6fcff68-9723-4d88-a200-525bb1791595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 21911, Validation size: 7304, Test size: 7304\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into train+validation and test sets\n",
    "train_val_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split train+validation into train and validation sets\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "\n",
    "print(f\"Train size: {len(train_data)}, Validation size: {len(val_data)}, Test size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a48677aa-def1-4693-b3b6-10e3f4069689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spliting_features_dependent_var(data):\n",
    "    X = data[\"Word\"]\n",
    "    y = data[\"Tag\"]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035f5922-3c36-4e82-b68c-93be1c52aded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef417a80-c9f9-41ff-86f2-f52d03ef0d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the datasets\n",
    "X_train, y_train = spliting_features_dependent_var(train_data)\n",
    "X_val, y_val = spliting_features_dependent_var(val_data)\n",
    "X_test, y_test = spliting_features_dependent_var(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86f9d45a-8283-4a3f-931b-9f709cc1d4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21911,), (21911,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1487c2dd-80e1-47ed-b6ea-8154cb46f819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7304,), (7304,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2a76633-c748-40a2-832b-7181af506330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7304,), (7304,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fef51cc-2b3a-4a47-8f21-46f989b1ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame({'Word': X_train, 'Tag': y_train})\n",
    "val_data = pd.DataFrame({'Word': X_val, 'Tag': y_val})\n",
    "test_data = pd.DataFrame({'Word': X_test, 'Tag': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fd28a83-44e4-41f0-84c4-ab7a819c8c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def preprocess_and_vectorize(data):\n",
    "    X, y = [], []\n",
    "    \n",
    "    # Iterate over each row in the dataframe\n",
    "    for index, row in data.iterrows():\n",
    "        word = row['Word']\n",
    "        tag = row['Tag']\n",
    "        \n",
    "        # Create a dictionary of features (only 'Word' in this case)\n",
    "        features = {'Word': word}\n",
    "        \n",
    "        # Append the feature dictionary and corresponding label to X and y\n",
    "        X.append(features)\n",
    "        y.append(tag)\n",
    "    \n",
    "    # Vectorize features using DictVectorizer\n",
    "    vec = DictVectorizer(sparse=False)\n",
    "    X_vectorized = vec.fit_transform(X)\n",
    "    \n",
    "    return X_vectorized, y, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f79dbd1d-6b20-447d-86d0-5c3df7832674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and vectorize training data\n",
    "X_train_vec, y_train_vec, vec = preprocess_and_vectorize(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "105898b1-a5a2-4961-ba3b-928b72a7b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform validation and test data using the same vectorizer\n",
    "X_val_vec = vec.transform(val_data.to_dict('records'))\n",
    "X_test_vec = vec.transform(test_data.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46d1a5d2-2cb0-4ab4-9408-c354969fda71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model on vectorized training data\n",
    "model.fit(X_train_vec, y_train_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbe9e728-2ddf-4224-9c64-4d1a2e999810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report on Validation Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00        42\n",
      "       B-eve       0.00      0.00      0.00        11\n",
      "       B-geo       0.00      0.00      0.00       519\n",
      "       B-gpe       0.00      0.00      0.00       102\n",
      "       B-nat       0.00      0.00      0.00         6\n",
      "       B-org       0.00      0.00      0.00       465\n",
      "       B-per       0.00      0.00      0.00       504\n",
      "       B-tim       0.00      0.00      0.00       189\n",
      "       I-art       0.00      0.00      0.00        43\n",
      "       I-eve       0.00      0.00      0.00        15\n",
      "       I-geo       0.00      0.00      0.00       171\n",
      "       I-gpe       0.00      0.00      0.00         8\n",
      "       I-nat       0.00      0.00      0.00         1\n",
      "       I-org       0.00      0.00      0.00       476\n",
      "       I-per       0.00      0.00      0.00       626\n",
      "       I-tim       0.00      0.00      0.00       107\n",
      "           O       0.55      1.00      0.71      4019\n",
      "\n",
      "    accuracy                           0.55      7304\n",
      "   macro avg       0.03      0.06      0.04      7304\n",
      "weighted avg       0.30      0.55      0.39      7304\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict on validation data\n",
    "y_pred_val = model.predict(X_val_vec)\n",
    "\n",
    "# Evaluate model performance on validation data\n",
    "print(\"Classification Report on Validation Data:\")\n",
    "print(classification_report(y_val, y_pred_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68898ea6-8a29-42ae-9ca1-ca9b8d35a148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00        46\n",
      "       B-eve       0.00      0.00      0.00        13\n",
      "       B-geo       0.00      0.00      0.00       474\n",
      "       B-gpe       0.00      0.00      0.00        79\n",
      "       B-nat       0.00      0.00      0.00         8\n",
      "       B-org       0.00      0.00      0.00       465\n",
      "       B-per       0.00      0.00      0.00       500\n",
      "       B-tim       0.00      0.00      0.00       161\n",
      "       I-art       0.00      0.00      0.00        38\n",
      "       I-eve       0.00      0.00      0.00        14\n",
      "       I-geo       0.00      0.00      0.00       170\n",
      "       I-gpe       0.00      0.00      0.00        10\n",
      "       I-nat       0.00      0.00      0.00         3\n",
      "       I-org       0.00      0.00      0.00       493\n",
      "       I-per       0.00      0.00      0.00       639\n",
      "       I-tim       0.00      0.00      0.00       118\n",
      "           O       0.56      1.00      0.72      4073\n",
      "\n",
      "    accuracy                           0.56      7304\n",
      "   macro avg       0.03      0.06      0.04      7304\n",
      "weighted avg       0.31      0.56      0.40      7304\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "y_pred_test = model.predict(X_test_vec)\n",
    "\n",
    "# Evaluate model performance on test data\n",
    "print(\"Classification Report on Test Data:\")\n",
    "print(classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37b2d72f-14a8-4e16-8c6f-9d931327b46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Word Predicted_Tag\n",
      "0    Germany             O\n",
      "1         is             O\n",
      "2          a             O\n",
      "3  beautiful             O\n",
      "4    country             O\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def predict_tags(model, vec, new_words):\n",
    "    \"\"\"\n",
    "    Function to predict Named Entity Recognition tags for new words.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained machine learning model (e.g., Logistic Regression)\n",
    "    - vec: DictVectorizer instance used for vectorizing training data\n",
    "    - new_words: List or Series of new words to predict tags for\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame containing 'Word' and 'Predicted_Tag' columns\n",
    "    \"\"\"\n",
    "    # Prepare new data in a DataFrame format\n",
    "    new_data = pd.DataFrame({'Word': new_words})\n",
    "    \n",
    "    # Vectorize the new data using the same DictVectorizer instance\n",
    "    X_new = vec.transform(new_data.to_dict('records'))\n",
    "    \n",
    "    # Make predictions using the trained model\n",
    "    y_pred_new = model.predict(X_new)\n",
    "    \n",
    "    # Create a DataFrame to display predictions\n",
    "    predictions_df = pd.DataFrame({'Word': new_words, 'Predicted_Tag': y_pred_new})\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `model` is your trained Logistic Regression model\n",
    "# Assuming `vec` is the DictVectorizer instance used for training data\n",
    "\n",
    "# List of new words to predict tags for\n",
    "new_words = ['Germany', 'is', 'a', 'beautiful', 'country']\n",
    "\n",
    "# Predict tags for the new words\n",
    "predictions = predict_tags(model, vec, new_words)\n",
    "\n",
    "# Display predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3923fd-35d8-4df7-b336-18150f03943a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88882406-e178-4972-88fb-1837a15706af",
   "metadata": {},
   "source": [
    "### Approach 2: Deep Learning Algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9zm9JZL9XUy",
   "metadata": {
    "id": "f9zm9JZL9XUy"
   },
   "source": [
    "### The Algorithm: BiLSTM for NER\n",
    "In this example, we use a Bidirectional Long Short-Term Memory (BiLSTM) network for NER. Let's understand the key concepts.\n",
    "\n",
    "#### 1. Long Short-Term Memory (LSTM)\n",
    "LSTM: A type of Recurrent Neural Network (RNN) designed to remember information for long periods. Unlike regular RNNs, LSTMs can learn and retain long-range dependencies, making them effective for sequence prediction tasks.\n",
    "\n",
    "#### 2. Bidirectional LSTM (BiLSTM)\n",
    "Bidirectional: In a BiLSTM, we have two LSTMs for each time step, one processing the sequence from the start to the end (forward direction) and the other from the end to the start (backward direction). This allows the model to have both past and future context, which is useful for understanding the meaning of each word in a sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7At3gfcy9xsg",
   "metadata": {
    "id": "7At3gfcy9xsg"
   },
   "source": [
    "### The Process: Training a BiLSTM Model for NER\n",
    "\n",
    "**(A) Data Preprocessing**\n",
    "\n",
    "(1) Tokenization:\n",
    "\n",
    "Splitting text into individual words.\n",
    "\n",
    "\n",
    "(2) Mapping to Indices:\n",
    "\n",
    "Converting words and tags into numerical indices that the model can understand.\n",
    "\n",
    "(3)Padding:\n",
    "\n",
    "Ensuring all sentences have the same length by adding \"padding\" tokens to shorter sentences and truncating longer ones.\n",
    "\n",
    "**(B) Model Building**\n",
    "\n",
    "(2) Embedding Layer:\n",
    "\n",
    "Converts each word into a dense vector of fixed size. These vectors capture semantic information about the words.\n",
    "\n",
    "(2) BiLSTM Layer:\n",
    "\n",
    "Processes the input sequences in both forward and backward directions.\n",
    "\n",
    "(3) TimeDistributed Layer:\n",
    "\n",
    "Applies a dense layer to each time step (word) independently, predicting the tag for each word.\n",
    "\n",
    "**(C) Model Training**\n",
    "\n",
    "(1) Compilation:\n",
    "\n",
    "Setting up the model with an optimizer (e.g., Adam), loss function (e.g., categorical crossentropy), and evaluation metric (e.g., accuracy).\n",
    "\n",
    "\n",
    "(B) Training: Fitting the model to the training data, adjusting weights to minimize the loss.\n",
    "\n",
    "\n",
    "**(D) Prediction and Evaluation**\n",
    "\n",
    "(1) Prediction: Using the trained model to predict tags for new sentences.\n",
    "\n",
    "(2) Evaluation: Assessing the model’s performance on a test dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qbtJEIS1-oJJ",
   "metadata": {
    "id": "qbtJEIS1-oJJ"
   },
   "source": [
    "### The Code\n",
    "\n",
    "Here's the full code with explanations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R2t8lT_wdWCO",
   "metadata": {
    "id": "R2t8lT_wdWCO"
   },
   "source": [
    "#### (A) Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "997090fb-9b52-4ebf-8275-3c336176b331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow)\n",
      "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (4.23.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (1.64.1)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow)\n",
      "  Downloading keras-3.3.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (1.26.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow)\n",
      "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: namex, libclang, flatbuffers, wrapt, tensorflow-io-gcs-filesystem, optree, opt-einsum, ml-dtypes, h5py, google-pasta, gast, astunparse, tensorboard, keras, tensorflow\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.15.1\n",
      "    Uninstalling tensorboard-2.15.1:\n",
      "      Successfully uninstalled tensorboard-2.15.1\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 h5py-3.11.0 keras-3.3.3 libclang-18.1.1 ml-dtypes-0.3.2 namex-0.0.8 opt-einsum-3.3.0 optree-0.11.0 tensorboard-2.16.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.37.0 wrapt-1.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71fba3a7-cd7e-4c09-9463-b10dfa334388",
   "metadata": {
    "id": "71fba3a7-cd7e-4c09-9463-b10dfa334388"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 06:15:06.332317: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-19 06:15:07.327769: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2cb6934-4c62-4751-9f1f-70384d6723a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "a2cb6934-4c62-4751-9f1f-70384d6723a3",
    "outputId": "d0d1b1c1-1134-45cf-d478-c1b5b70c36df"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data = pd.read_csv(\"ner_dataset.csv\",  encoding='latin1')\n",
    "ner_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2EVVcE--gsqZ",
   "metadata": {
    "id": "2EVVcE--gsqZ"
   },
   "source": [
    "Understanding the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "tkjjqSUMgqkH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tkjjqSUMgqkH",
    "outputId": "d2f05a4a-d6da-43b5-c95f-8eb9ca89e894"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048575, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bSMfgQuhtnJ",
   "metadata": {
    "id": "7bSMfgQuhtnJ"
   },
   "source": [
    "Group the senetences with its tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ohtnFd8Fp7dC",
   "metadata": {
    "id": "ohtnFd8Fp7dC"
   },
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                     s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "C8PpTNv1v06l",
   "metadata": {
    "id": "C8PpTNv1v06l"
   },
   "outputs": [],
   "source": [
    "getter = SentenceGetter(ner_data)\n",
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "G5TD-u6NvEW5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G5TD-u6NvEW5",
    "outputId": "96555146-3b6d-42ea-9824-b85731cb13f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47959"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y-kbi3G9p7aY",
   "metadata": {
    "id": "y-kbi3G9p7aY"
   },
   "outputs": [],
   "source": [
    "# Extract unique words and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "S4lEYv7OvqYG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S4lEYv7OvqYG",
    "outputId": "71ddb3df-a921-49d3-c11e-273f721a0a5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35179"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(set(ner_data[\"Word\"].values))\n",
    "words.append(\"ENDPAD\")\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "-Z1k6UCKvqT6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Z1k6UCKvqT6",
    "outputId": "5d0777d6-f06a-41e2-bd5f-29e5dc364664"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = list(set(ner_data[\"Tag\"].values))\n",
    "len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "--C4Kmt0thMu",
   "metadata": {
    "id": "--C4Kmt0thMu"
   },
   "outputs": [],
   "source": [
    "# Dictionary mapping words and tags to indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "SMTgMnu7vwiE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SMTgMnu7vwiE",
    "outputId": "b17decf9-de8e-45bc-f178-95fea60e27a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35179"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "au2nRJjsvwf5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "au2nRJjsvwf5",
    "outputId": "d1fc5132-dc26-4005-9b35-a3c15c4b4505"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "len(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fWWjjC0thIe",
   "metadata": {
    "id": "0fWWjjC0thIe"
   },
   "outputs": [],
   "source": [
    "# Prepare data for the model\n",
    "max_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cVgZlCDHvS_w",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cVgZlCDHvS_w",
    "outputId": "1abed7fd-56a5-4045-a7d8-1c77239ac599"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47959"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "len(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fmmikBWI61VZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fmmikBWI61VZ",
    "outputId": "2ed89715-3796-421b-dde6-9eb01249fd63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5374], [1536]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "CtNTKVW4vS82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CtNTKVW4vS82",
    "outputId": "73759be2-fb8f-43e8-9b6a-8de1dc24898c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5374, 35178, 35178, ..., 35178, 35178, 35178],\n",
       "       [ 1536, 35178, 35178, ..., 35178, 35178, 35178],\n",
       "       [11248, 35178, 35178, ..., 35178, 35178, 35178],\n",
       "       ...,\n",
       "       [ 5309, 35178, 35178, ..., 35178, 35178, 35178],\n",
       "       [21838, 35178, 35178, ..., 35178, 35178, 35178],\n",
       "       [18083, 35178, 35178, ..., 35178, 35178, 35178]], dtype=int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=word2idx[\"ENDPAD\"])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "GCuZLo1W4-88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GCuZLo1W4-88",
    "outputId": "96187f26-ce24-44ae-b89b-f2f426efec06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47959, 50)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "U1yoDOQuvS3k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1yoDOQuvS3k",
    "outputId": "dcc43fe5-104e-4928-8af8-c3d1058fdee9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47959"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "xOX6Ir40vS1T",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xOX6Ir40vS1T",
    "outputId": "bbb861bc-5560-4795-9ea7-d5611d3ee174"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  7,  7, ...,  7,  7,  7],\n",
       "       [16,  7,  7, ...,  7,  7,  7],\n",
       "       [ 7,  7,  7, ...,  7,  7,  7],\n",
       "       ...,\n",
       "       [ 7,  7,  7, ...,  7,  7,  7],\n",
       "       [ 7,  7,  7, ...,  7,  7,  7],\n",
       "       [ 7,  7,  7, ...,  7,  7,  7]], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "xfT4ViqsvXf-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xfT4ViqsvXf-",
    "outputId": "53c03108-d1fc-43cd-fe92-85b3f9eba14d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47959"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [to_categorical(i, num_classes=len(tags)) for i in y]\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aDgpLxh_thFz",
   "metadata": {
    "id": "aDgpLxh_thFz"
   },
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q5CQDqEK-4PV",
   "metadata": {
    "id": "Q5CQDqEK-4PV"
   },
   "source": [
    "Loading Data:\n",
    "\n",
    "Read the CSV file into a DataFrame and fill missing values.\n",
    "SentenceGetter: Groups words and tags by sentences.\n",
    "\n",
    "\n",
    "Mapping to Indices:\n",
    "\n",
    "Creates dictionaries to map words and tags to numerical indices.\n",
    "\n",
    "Padding and Encoding:\n",
    "\n",
    "Converts sentences to fixed-length sequences of indices and encodes tags as one-hot vectors.\n",
    "\n",
    "Splitting Data:\n",
    "\n",
    " Splits the dataset into training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ENA43ea6-_F-",
   "metadata": {
    "id": "ENA43ea6-_F-"
   },
   "source": [
    "#### (B) Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "Z9tdHqjathDC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z9tdHqjathDC",
    "outputId": "5321e186-b5ef-4d41-854d-500bb4825565"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(words), output_dim=50, input_length=max_len),\n",
    "    Dropout(0.1),\n",
    "    Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1)),\n",
    "    TimeDistributed(Dense(len(tags), activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ixBFzDxv_LxV",
   "metadata": {
    "id": "ixBFzDxv_LxV"
   },
   "source": [
    "Embedding Layer:\n",
    "\n",
    "Converts words to dense vectors.\n",
    "\n",
    "\n",
    "BiLSTM Layer:\n",
    "\n",
    "Processes sequences in both forward and backward directions.\n",
    "\n",
    "TimeDistributed Layer:\n",
    "\n",
    "Applies a dense layer to each word to predict its tag.\n",
    "\n",
    "Compilation:\n",
    "\n",
    "Sets up the optimizer, loss function, and metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x-Ye7mkf_THy",
   "metadata": {
    "id": "x-Ye7mkf_THy"
   },
   "source": [
    "#### (C) Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ji8jQzBg_cp0",
   "metadata": {
    "id": "Ji8jQzBg_cp0"
   },
   "source": [
    "This step will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "tAr3qzjBthAl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tAr3qzjBthAl",
    "outputId": "780e828e-7264-43bd-97ab-54e101e3d6cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1214/1214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 57ms/step - accuracy: 0.9894 - loss: 0.0900 - val_accuracy: 0.9977 - val_loss: 0.0073\n",
      "Epoch 2/5\n",
      "\u001b[1m1214/1214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 56ms/step - accuracy: 0.9983 - loss: 0.0058 - val_accuracy: 0.9985 - val_loss: 0.0048\n",
      "Epoch 3/5\n",
      "\u001b[1m1214/1214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 56ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 0.9985 - val_loss: 0.0045\n",
      "Epoch 4/5\n",
      "\u001b[1m1214/1214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 56ms/step - accuracy: 0.9991 - loss: 0.0030 - val_accuracy: 0.9986 - val_loss: 0.0045\n",
      "Epoch 5/5\n",
      "\u001b[1m1214/1214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 56ms/step - accuracy: 0.9991 - loss: 0.0028 - val_accuracy: 0.9986 - val_loss: 0.0045\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, np.array(y_train), batch_size=32, epochs=5, validation_split=0.1, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y1Y46n4u_bMn",
   "metadata": {
    "id": "Y1Y46n4u_bMn"
   },
   "source": [
    "Training:\n",
    "\n",
    "Fits the model to the training data, using a batch size of 32 and training for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "PS102hqsp7Xp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PS102hqsp7Xp",
    "outputId": "1eb4f155-7012-42b6-de49-1e470cc16970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9986 - loss: 0.0045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0044597783125936985, 0.9985813498497009]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(X_test, np.array(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uQ8icT6ziom9",
   "metadata": {
    "id": "uQ8icT6ziom9"
   },
   "source": [
    "Loss (0.046321723610162735):\n",
    "\n",
    "This value represents the model's loss on the test set. In this context, the loss is calculated using the categorical cross-entropy loss function, which measures the difference between the predicted and true probability distributions. A lower loss value indicates that the model's predictions are closer to the actual tags. The value 0.0463 indicates that the model has a relatively low error in its predictions on the test set.\n",
    "Accuracy (0.9860008358955383):\n",
    "\n",
    "This value represents the model's accuracy on the test set. Accuracy is the fraction of correct predictions made by the model. In this case, the value 0.9860 indicates that the model correctly predicted the NER tags for 98.60% of the words in the test set. This is a high accuracy, suggesting that the model is performing well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WddfoZYv_iV-",
   "metadata": {
    "id": "WddfoZYv_iV-"
   },
   "source": [
    "#### (D) Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "Xj4yMQJ32PMW",
   "metadata": {
    "id": "Xj4yMQJ32PMW"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def predict_tags(sentence, tags, word2idx, max_len, model):\n",
    "    words = sentence.split()\n",
    "    seq = pad_sequences([[word2idx.get(w, word2idx[\"ENDPAD\"]) for w in words]], maxlen=max_len, padding=\"post\", value=word2idx[\"ENDPAD\"])\n",
    "    preds = model.predict(seq)\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "    predicted_tags = [tags[i] for i in preds[0]]\n",
    "    predictions=  list(zip(words, predicted_tags[:len(words)]))\n",
    "    df_predictions = pd.DataFrame(predictions, columns=[\"Word\", \"Tag\"])\n",
    "\n",
    "    # Display the DataFrame as a table\n",
    "    display(HTML(df_predictions.to_html(index=False)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l4IndboE_zE0",
   "metadata": {
    "id": "l4IndboE_zE0"
   },
   "source": [
    "Predict Tags:\n",
    "\n",
    "Tokenizes the input sentence, converts it to indices, and pads it to the maximum length. The model predicts tags for each word, which are then converted back to their original form.\n",
    "\n",
    "Display Results:\n",
    "\n",
    "Creates a DataFrame from the predictions and displays it as a nicely formatted table in Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VQz3wUDp_pZy",
   "metadata": {
    "id": "VQz3wUDp_pZy"
   },
   "source": [
    "Let's make some predictions on new sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "wRZR0p8C7gaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "wRZR0p8C7gaf",
    "outputId": "774d2c75-8b76-4ab5-a191-19920d112186"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 564ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>India</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>best</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>place</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>to</td>\n",
       "      <td>B-per</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>live.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = \"India is the best place to live.\"\n",
    "predictions = predict_tags(sentence, tags, word2idx, max_len, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "LILyLf1j2qnF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "LILyLf1j2qnF",
    "outputId": "fefe2a41-581d-4c16-c7b7-4cda6cc6f150"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>European</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Union</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>biggest</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>organisation.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence_2 = \"European Union is the biggest organisation.\"\n",
    "\n",
    "predictions = predict_tags(sentence_2, tags, word2idx, max_len, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KrBZ2mIvpy-V",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "KrBZ2mIvpy-V",
    "outputId": "d94c9c5a-c952-42c8-a13c-cd9ed99e3c4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>In</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Germany</td>\n",
       "      <td>B-org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Nigeria,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>there</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>are</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>lot</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>other</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>things</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>which</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>are</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>not</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>that</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>good.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence_3 = \"In Germany and Nigeria, there are lot of other things which are not that good.\"\n",
    "\n",
    "predictions = predict_tags(sentence_3, tags, word2idx, max_len, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "hOxE1Gt3py74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "id": "hOxE1Gt3py74",
    "outputId": "dbb4a7f8-9a46-4ca1-86ee-fd66540f187b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Mosul</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>and</td>\n",
       "      <td>B-org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Suresh</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>were</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>best</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>friends</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>when</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>they</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>were</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Baghdad.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence_4 = \"Mosul and Suresh were best friends when they were in Baghdad.\"\n",
    "\n",
    "predictions = predict_tags(sentence_4, tags, word2idx, max_len, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d940c91-e982-4bce-8a4e-7f1b4800ad20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Germany</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>a</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>beautiful</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>country..</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence_5 = \"Germany is a beautiful country..\"\n",
    "\n",
    "predictions = predict_tags(sentence_5, tags, word2idx, max_len, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3530d574-f203-48e4-93a2-3a91e0cd3ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Eiffel</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Tower</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Paris,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>France,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>a</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>famous</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>tourist</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>attraction.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence_6 = \"The Eiffel Tower in Paris, France, is a famous tourist attraction.\"\n",
    "predictions = predict_tags(sentence_6, tags, word2idx, max_len, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pjD_iPkX_6ud",
   "metadata": {
    "id": "pjD_iPkX_6ud"
   },
   "source": [
    "### Summary :\n",
    "\n",
    "Preprocessing:\n",
    "\n",
    "Prepare data by tokenizing, encoding, and padding sentences.\n",
    "Model Building: Build a BiLSTM model using Tensorflow.\n",
    "\n",
    "Training: Train the model on the preprocessed data.\n",
    "\n",
    "Prediction: Predict NER tags for new sentences and display results in a tabular format.\n",
    "\n",
    "By following these steps, we can effectively use a BiLSTM model for Named Entity Recognition, enabling us to identify and classify entities in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7e1675db-d5ce-4126-8388-a09e6a5ea914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Russia</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>and</td>\n",
       "      <td>B-org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>China</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>signed</td>\n",
       "      <td>B-org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>a</td>\n",
       "      <td>B-org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>new</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>trade</td>\n",
       "      <td>B-per</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>agreement.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence_8 = \"Russia and China signed a new trade agreement.\"\n",
    "predictions = predict_tags(sentence_8, tags, word2idx, max_len, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eePstzoqyO8",
   "metadata": {
    "id": "9eePstzoqyO8"
   },
   "source": [
    "### Future Steps\n",
    "\n",
    "1. Use Pre-trained Embeddings:\n",
    "\n",
    "Incorporate GloVe or BERT embeddings to improve performance.\n",
    "\n",
    "2. Hyperparameter Tuning:\n",
    "\n",
    "Experiment with different hyperparameters like batch size, learning rate, number of LSTM units, etc.\n",
    "\n",
    "\n",
    "3. Ensemble Methods:\n",
    "\n",
    "Combine predictions from multiple models to improve accuracy.\n",
    "\n",
    "\n",
    "4. Error Analysis:\n",
    "\n",
    "Analyze errors to understand common failure cases and address them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HQC01fO_31-T",
   "metadata": {
    "id": "HQC01fO_31-T"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4JzG11njrE1V",
   "metadata": {
    "id": "4JzG11njrE1V"
   },
   "source": [
    "### Approach 3 : Use Pre-trained Embeddings\n",
    "\n",
    "We'll start by incorporating pre-trained GloVe embeddings into our model to improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "IqiNXqYurGnu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IqiNXqYurGnu",
    "outputId": "f1b4331b-5bd9-4f4d-a0b1-91b42e3933a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-19 06:29:15--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2024-06-19 06:29:15--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2024-06-19 06:29:16--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
      "\n",
      "2024-06-19 06:31:55 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "75Z8tnGMrKRd",
   "metadata": {
    "id": "75Z8tnGMrKRd"
   },
   "outputs": [],
   "source": [
    "# Load the embeddings\n",
    "embedding_index = {}\n",
    "with open(\"glove.6B.100d.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(words), embedding_dim))\n",
    "for word, i in word2idx.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4yPFi4IJrXoi",
   "metadata": {
    "id": "4yPFi4IJrXoi"
   },
   "source": [
    "Build Model with GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "tKYyJYpzrSbO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tKYyJYpzrSbO",
    "outputId": "3004f777-cacb-46f2-92b3-05c38b0c47b5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,517,900</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_1              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │     \u001b[38;5;34m3,517,900\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_1              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,517,900</span> (13.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,517,900\u001b[0m (13.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,517,900</span> (13.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,517,900\u001b[0m (13.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "model_glove = Sequential([\n",
    "    Embedding(input_dim=len(words), output_dim=embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=False),\n",
    "    Dropout(0.1),\n",
    "    Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1)),\n",
    "    TimeDistributed(Dense(len(tags), activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "model_glove.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_glove.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EIiijuUormyV",
   "metadata": {
    "id": "EIiijuUormyV"
   },
   "source": [
    "Train the Model with GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "NcOr0UgUrSY8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NcOr0UgUrSY8",
    "outputId": "0b14f126-0571-4287-c6c6-54e165760392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1349/1349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 46ms/step - accuracy: 0.9885 - loss: 0.1691\n",
      "Epoch 2/5\n",
      "\u001b[1m1349/1349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 46ms/step - accuracy: 0.9943 - loss: 0.0210\n",
      "Epoch 3/5\n",
      "\u001b[1m1349/1349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 46ms/step - accuracy: 0.9942 - loss: 0.0212\n",
      "Epoch 4/5\n",
      "\u001b[1m1349/1349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 47ms/step - accuracy: 0.9943 - loss: 0.0206\n",
      "Epoch 5/5\n",
      "\u001b[1m1349/1349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 47ms/step - accuracy: 0.9943 - loss: 0.0206\n"
     ]
    }
   ],
   "source": [
    "history_glove = model_glove.fit(X_train, np.array(y_train), batch_size=32, epochs=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "QmbJ5MenzPGn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QmbJ5MenzPGn",
    "outputId": "fd625f04-e6a7-4f47-c04b-b543cb662314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9945 - loss: 0.0201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.02022641897201538, 0.9944704174995422]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model_glove.evaluate(X_test, np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "zCZB_lvbrrTn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "zCZB_lvbrrTn",
    "outputId": "82a5d834-9245-4430-bf68-284e9f683b9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 547ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Germany</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>one</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>main</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>economy</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>world.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence_1 = \"Germany is one of the main economy in the world.\"\n",
    "predictions = predict_tags(sentence_1, tags, word2idx, max_len, model_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "IrFV7V7CrrRb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "IrFV7V7CrrRb",
    "outputId": "63a2a3cd-8e44-4fac-a826-0c8b66350d8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>In</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Germany</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Nigeria,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>there</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>are</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>lot</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>other</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>things</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>which</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>are</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>not</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>that</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>good.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence_2 = \"In Germany and Nigeria, there are lot of other things which are not that good.\"\n",
    "predictions = predict_tags(sentence_2, tags, word2idx, max_len, model_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NtExxiyj0u0V",
   "metadata": {
    "id": "NtExxiyj0u0V"
   },
   "source": [
    "### Approach 4 : Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l-YzhKAVrqKh",
   "metadata": {
    "id": "l-YzhKAVrqKh"
   },
   "source": [
    "\n",
    "\n",
    "We will tune hyperparameters such as batch size, learning rate, and the number of LSTM units. We can use tools like Keras Tuner, but for simplicity, let's manually experiment with different configurations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hEZUUEQ_r25Y",
   "metadata": {
    "id": "hEZUUEQ_r25Y"
   },
   "source": [
    "Define a Function to Build the Model with Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7oz-TzxArSWL",
   "metadata": {
    "id": "7oz-TzxArSWL"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model(embedding_matrix, lstm_units=100, dropout_rate=0.1, learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=len(words), output_dim=embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=False),\n",
    "        Dropout(dropout_rate),\n",
    "        Bidirectional(LSTM(units=lstm_units, return_sequences=True, recurrent_dropout=dropout_rate)),\n",
    "        TimeDistributed(Dense(len(tags), activation=\"softmax\"))\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5noRLVNcsRMS",
   "metadata": {
    "id": "5noRLVNcsRMS"
   },
   "source": [
    "Train and Evaluate Models with Different Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "E2q5VpTMrSQg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E2q5VpTMrSQg",
    "outputId": "a5ed51dd-e2fd-4030-e4af-19f06be12f97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - accuracy: 0.9838 - loss: 0.3712\n",
      "Epoch 2/5\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 40ms/step - accuracy: 0.9943 - loss: 0.0260\n",
      "Epoch 3/5\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 40ms/step - accuracy: 0.9943 - loss: 0.0223\n",
      "Epoch 4/5\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 40ms/step - accuracy: 0.9943 - loss: 0.0210\n",
      "Epoch 5/5\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 40ms/step - accuracy: 0.9944 - loss: 0.0205\n"
     ]
    }
   ],
   "source": [
    "# Example configuration 1\n",
    "model_hp1 = build_model(embedding_matrix, lstm_units=50, dropout_rate=0.2, learning_rate=0.001)\n",
    "history_hp1 = model_hp1.fit(X_train, np.array(y_train), batch_size=64, epochs=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4HbrW_d11GZE",
   "metadata": {
    "id": "4HbrW_d11GZE"
   },
   "source": [
    "We can have multiple configurations like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "MBiyvGQIz8Or",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBiyvGQIz8Or",
    "outputId": "71249262-53c8-44d1-8a9b-466f89077ee7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9945 - loss: 0.0202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.020287295803427696, 0.9944704174995422]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model_hp1.evaluate(X_test, np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "N9mgnQv_z8Lp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "N9mgnQv_z8Lp",
    "outputId": "83e8fcd3-0cb7-42c9-9d22-4f90aaa7f1d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 553ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Germany</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>one</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>main</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>economy</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>world.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence_1 = \"Germany is one of the main economy in the world.\"\n",
    "predictions = predict_tags(sentence_1, tags, word2idx, max_len, model_hp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N_yil1rysWG9",
   "metadata": {
    "id": "N_yil1rysWG9"
   },
   "source": [
    "Repeat this for other configurations and compare the validation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uzS3-9Ii0z4O",
   "metadata": {
    "id": "uzS3-9Ii0z4O"
   },
   "source": [
    "### Approach 5 : Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FX1Nd64Hsab7",
   "metadata": {
    "id": "FX1Nd64Hsab7"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Combining predictions from multiple models can improve accuracy. We'll average the probabilities from different models.\n",
    "\n",
    "Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "gFz05n-RrSNv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gFz05n-RrSNv",
    "outputId": "7124421d-e7a0-424c-96b9-b6946b28be19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1349/1349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 46ms/step - accuracy: 0.9885 - loss: 0.1673\n",
      "Epoch 2/5\n",
      "\u001b[1m1349/1349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 46ms/step - accuracy: 0.9943 - loss: 0.0212\n",
      "Epoch 3/5\n",
      "\u001b[1m1349/1349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 46ms/step - accuracy: 0.9943 - loss: 0.0206\n",
      "Epoch 4/5\n",
      "\u001b[1m1349/1349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 46ms/step - accuracy: 0.9943 - loss: 0.0206\n",
      "Epoch 5/5\n",
      "\u001b[1m1349/1349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 46ms/step - accuracy: 0.9943 - loss: 0.0206\n"
     ]
    }
   ],
   "source": [
    "# Example model 1\n",
    "model1 = build_model(embedding_matrix, lstm_units=100, dropout_rate=0.1, learning_rate=0.001)\n",
    "history1 = model1.fit(X_train, np.array(y_train), batch_size=32, epochs=5, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PPcBhgVX1NYC",
   "metadata": {
    "id": "PPcBhgVX1NYC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1349/1349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 67ms/step - accuracy: 0.9885 - loss: 0.1423\n",
      "Epoch 2/5\n",
      "\u001b[1m1349/1349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 68ms/step - accuracy: 0.9943 - loss: 0.0209\n",
      "Epoch 3/5\n",
      "\u001b[1m1349/1349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 68ms/step - accuracy: 0.9943 - loss: 0.0207\n",
      "Epoch 4/5\n",
      "\u001b[1m1349/1349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 68ms/step - accuracy: 0.9943 - loss: 0.0207\n",
      "Epoch 5/5\n",
      "\u001b[1m 344/1349\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 68ms/step - accuracy: 0.9941 - loss: 0.0210"
     ]
    }
   ],
   "source": [
    "# Example model 2\n",
    "model2 = build_model(embedding_matrix, lstm_units=150, dropout_rate=0.2, learning_rate=0.001)\n",
    "history2 = model2.fit(X_train, np.array(y_train), batch_size=32, epochs=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sYoD_zLqspsl",
   "metadata": {
    "id": "sYoD_zLqspsl"
   },
   "source": [
    "\n",
    "Ensemble Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A1cv2mTOrSK-",
   "metadata": {
    "id": "A1cv2mTOrSK-"
   },
   "outputs": [],
   "source": [
    "def ensemble_predict(models, sentence, tags, word2idx, max_len):\n",
    "    words = sentence.split()\n",
    "    seq = pad_sequences([[word2idx.get(w, word2idx[\"ENDPAD\"]) for w in words]], maxlen=max_len, padding=\"post\", value=word2idx[\"ENDPAD\"])\n",
    "\n",
    "    # Sum predictions from all models\n",
    "    total_preds = np.zeros((1, max_len, len(tags)))\n",
    "    for model in models:\n",
    "        preds = model.predict(seq)\n",
    "        total_preds += preds\n",
    "\n",
    "    # Average predictions\n",
    "    avg_preds = total_preds / len(models)\n",
    "    avg_preds = np.argmax(avg_preds, axis=-1)\n",
    "    predicted_tags = [tags[i] for i in avg_preds[0]]\n",
    "    return list(zip(words, predicted_tags[:len(words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HHr54QznstXj",
   "metadata": {
    "id": "HHr54QznstXj"
   },
   "outputs": [],
   "source": [
    "# Ensemble prediction\n",
    "sentence = \"Mark and John are good friends from London.\"\n",
    "models = [model1, model2]\n",
    "predictions = ensemble_predict(models, sentence, tags, word2idx, max_len)\n",
    "\n",
    "# Display results\n",
    "df_predictions = pd.DataFrame(predictions, columns=[\"Word\", \"Tag\"])\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(df_predictions.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8vxYLqDtsyV0",
   "metadata": {
    "id": "8vxYLqDtsyV0"
   },
   "source": [
    "### Approach 6 : Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GHkNQHVTs19q",
   "metadata": {
    "id": "GHkNQHVTs19q"
   },
   "source": [
    "##### Identify errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o7IuiK6xstVA",
   "metadata": {
    "id": "o7IuiK6xstVA"
   },
   "outputs": [],
   "source": [
    "def evaluate_and_analyze(model, X_test, y_test, idx2tag):\n",
    "    preds = model.predict(X_test)\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "    y_true = np.argmax(y_test, axis=-1)\n",
    "\n",
    "    errors = []\n",
    "    for i in range(len(y_true)):\n",
    "        for j in range(len(y_true[i])):\n",
    "            if y_true[i][j] != preds[i][j] and y_true[i][j] != 0:\n",
    "                errors.append((i, j, idx2tag[y_true[i][j]], idx2tag[preds[i][j]]))\n",
    "\n",
    "    return errors\n",
    "\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "errors = evaluate_and_analyze(model_glove, X_test, y_test, idx2tag)\n",
    "\n",
    "# Display errors\n",
    "error_df = pd.DataFrame(errors, columns=[\"Sentence Index\", \"Word Index\", \"True Tag\", \"Predicted Tag\"])\n",
    "display(HTML(error_df.to_html(index=False)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9596ba-7a24-4fa4-b1a0-fecf3812be33",
   "metadata": {},
   "source": [
    "### Approach 7:  Using Large Language Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf6def27-40ec-4902-b99e-a78ed3962e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: transformers[torch]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install transformers datasets evaluate transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f174f1c-a3f3-4357-821e-be0a3219dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Creating HuggingFace Dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65420591-55d8-428d-b883-1bb88f101cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ner_dataset.csv\",encoding='latin1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a16b624-94db-43b9-bf93-5d94402bb112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1546/2295021360.py:7: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data['Sentence #'].fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example data structure:\n",
    "# data = pd.read_csv('data.csv')\n",
    "\n",
    "# Fill NaN values in 'Sentence #' column with appropriate values\n",
    "data['Sentence #'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Replace NaN values in other columns (if any)\n",
    "data.fillna('', inplace=True)\n",
    "\n",
    "# Initialize variables to store sentences and tags\n",
    "sentences = []\n",
    "tags = []\n",
    "\n",
    "# Group by 'Sentence #' and iterate through groups\n",
    "for sentence_id, group in data.groupby('Sentence #'):\n",
    "    # Concatenate words to form the sentence\n",
    "    sentence = ' '.join(group['Word'].tolist())\n",
    "    \n",
    "    # Create a dictionary to store tags for the sentence\n",
    "    sentence_tags = {}\n",
    "    \n",
    "    # Iterate through each word and its corresponding tag in the group\n",
    "    for word, tag in zip(group['Word'], group['Tag']):\n",
    "        sentence_tags[word] = tag\n",
    "    \n",
    "    # Append sentence and its tags to lists\n",
    "    sentences.append(sentence)\n",
    "    tags.append(sentence_tags)\n",
    "\n",
    "# Create a new DataFrame\n",
    "df_new = pd.DataFrame({\n",
    "    'sentence': sentences,\n",
    "    'tag': tags\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74241785-f679-4a8f-99f5-9c9ae17ea8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv(\"dataset_for_llm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cea61f0-f731-4916-951c-c49790e7b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset_for_llm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35acdfc0-f3b6-41ce-b381-724b30c2e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d53a9f49-013e-40e4-a02f-22e03e6b59ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>{'Thousands': 'O', 'of': 'O', 'demonstrators':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iranian officials say they expect to get acces...</td>\n",
       "      <td>{'Iranian': 'B-gpe', 'officials': 'O', 'say': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Helicopter gunships Saturday pounded militant ...</td>\n",
       "      <td>{'Helicopter': 'O', 'gunships': 'O', 'Saturday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They left after a tense hour-long standoff wit...</td>\n",
       "      <td>{'They': 'O', 'left': 'O', 'after': 'O', 'a': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.N. relief coordinator Jan Egeland said Sunda...</td>\n",
       "      <td>{'U.N.': 'B-geo', 'relief': 'O', 'coordinator'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47954</th>\n",
       "      <td>Opposition leader Mir Hossein Mousavi has said...</td>\n",
       "      <td>{'Opposition': 'O', 'leader': 'O', 'Mir': 'O',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47955</th>\n",
       "      <td>On Thursday , Iranian state media published a ...</td>\n",
       "      <td>{'On': 'O', 'Thursday': 'B-tim', ',': 'O', 'Ir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47956</th>\n",
       "      <td>Following Iran 's disputed June 12 elections ,...</td>\n",
       "      <td>{'Following': 'O', 'Iran': 'B-geo', \"'s\": 'O',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47957</th>\n",
       "      <td>Since then , authorities have held public tria...</td>\n",
       "      <td>{'Since': 'O', 'then': 'O', ',': 'O', 'authori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47958</th>\n",
       "      <td>The United Nations is praising the use of mili...</td>\n",
       "      <td>{'The': 'O', 'United': 'B-org', 'Nations': 'I-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47959 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "0      Thousands of demonstrators have marched throug...   \n",
       "1      Iranian officials say they expect to get acces...   \n",
       "2      Helicopter gunships Saturday pounded militant ...   \n",
       "3      They left after a tense hour-long standoff wit...   \n",
       "4      U.N. relief coordinator Jan Egeland said Sunda...   \n",
       "...                                                  ...   \n",
       "47954  Opposition leader Mir Hossein Mousavi has said...   \n",
       "47955  On Thursday , Iranian state media published a ...   \n",
       "47956  Following Iran 's disputed June 12 elections ,...   \n",
       "47957  Since then , authorities have held public tria...   \n",
       "47958  The United Nations is praising the use of mili...   \n",
       "\n",
       "                                                     tag  \n",
       "0      {'Thousands': 'O', 'of': 'O', 'demonstrators':...  \n",
       "1      {'Iranian': 'B-gpe', 'officials': 'O', 'say': ...  \n",
       "2      {'Helicopter': 'O', 'gunships': 'O', 'Saturday...  \n",
       "3      {'They': 'O', 'left': 'O', 'after': 'O', 'a': ...  \n",
       "4      {'U.N.': 'B-geo', 'relief': 'O', 'coordinator'...  \n",
       "...                                                  ...  \n",
       "47954  {'Opposition': 'O', 'leader': 'O', 'Mir': 'O',...  \n",
       "47955  {'On': 'O', 'Thursday': 'B-tim', ',': 'O', 'Ir...  \n",
       "47956  {'Following': 'O', 'Iran': 'B-geo', \"'s\": 'O',...  \n",
       "47957  {'Since': 'O', 'then': 'O', ',': 'O', 'authori...  \n",
       "47958  {'The': 'O', 'United': 'B-org', 'Nations': 'I-...  \n",
       "\n",
       "[47959 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3b0c9c5-b1f5-4a3a-a4b3-4944858677c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "train_df, test_df = train_test_split(data, test_size=0.3, random_state=42)\n",
    "valid_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Reset index\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9d2f9c8-9eac-4597-b7fe-f41bf5929c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mousavi 's website also quotes him as criticiz...</td>\n",
       "      <td>{'Mousavi': 'B-per', \"'s\": 'O', 'website': 'O'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The billboards contain photographs of hooded I...</td>\n",
       "      <td>{'The': 'O', 'billboards': 'O', 'contain': 'O'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Uganda is the only country so far to agree to ...</td>\n",
       "      <td>{'Uganda': 'B-org', 'is': 'O', 'the': 'O', 'on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fighting between the Popular Movement for the ...</td>\n",
       "      <td>{'Fighting': 'O', 'between': 'O', 'the': 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meanwhile , officials in Ukraine have reported...</td>\n",
       "      <td>{'Meanwhile': 'O', ',': 'O', 'officials': 'O',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33566</th>\n",
       "      <td>During an address Wednesday marking the Muslim...</td>\n",
       "      <td>{'During': 'O', 'an': 'O', 'address': 'O', 'We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33567</th>\n",
       "      <td>General Abizaid made the remarks during a brie...</td>\n",
       "      <td>{'General': 'B-org', 'Abizaid': 'I-org', 'made...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33568</th>\n",
       "      <td>Milosevic had been on trial at the United Nati...</td>\n",
       "      <td>{'Milosevic': 'B-per', 'had': 'O', 'been': 'O'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33569</th>\n",
       "      <td>Lieberman introduced the bill with Republican ...</td>\n",
       "      <td>{'Lieberman': 'B-per', 'introduced': 'O', 'the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33570</th>\n",
       "      <td>She said divulging the contents of the negotia...</td>\n",
       "      <td>{'She': 'O', 'said': 'O', 'divulging': 'O', 't...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "0      Mousavi 's website also quotes him as criticiz...   \n",
       "1      The billboards contain photographs of hooded I...   \n",
       "2      Uganda is the only country so far to agree to ...   \n",
       "3      Fighting between the Popular Movement for the ...   \n",
       "4      Meanwhile , officials in Ukraine have reported...   \n",
       "...                                                  ...   \n",
       "33566  During an address Wednesday marking the Muslim...   \n",
       "33567  General Abizaid made the remarks during a brie...   \n",
       "33568  Milosevic had been on trial at the United Nati...   \n",
       "33569  Lieberman introduced the bill with Republican ...   \n",
       "33570  She said divulging the contents of the negotia...   \n",
       "\n",
       "                                                     tag  \n",
       "0      {'Mousavi': 'B-per', \"'s\": 'O', 'website': 'O'...  \n",
       "1      {'The': 'O', 'billboards': 'O', 'contain': 'O'...  \n",
       "2      {'Uganda': 'B-org', 'is': 'O', 'the': 'O', 'on...  \n",
       "3      {'Fighting': 'O', 'between': 'O', 'the': 'O', ...  \n",
       "4      {'Meanwhile': 'O', ',': 'O', 'officials': 'O',...  \n",
       "...                                                  ...  \n",
       "33566  {'During': 'O', 'an': 'O', 'address': 'O', 'We...  \n",
       "33567  {'General': 'B-org', 'Abizaid': 'I-org', 'made...  \n",
       "33568  {'Milosevic': 'B-per', 'had': 'O', 'been': 'O'...  \n",
       "33569  {'Lieberman': 'B-per', 'introduced': 'O', 'the...  \n",
       "33570  {'She': 'O', 'said': 'O', 'divulging': 'O', 't...  \n",
       "\n",
       "[33571 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f302a8f-973b-4e95-a94f-00ae6ade955c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1784 , the French sold the island to Sweden...</td>\n",
       "      <td>{'In': 'O', '1784': 'B-tim', ',': 'O', 'the': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Democrat Tom Daschle contradicts President Bus...</td>\n",
       "      <td>{'Democrat': 'O', 'Tom': 'B-per', 'Daschle': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Polls indicate that if early elections are hel...</td>\n",
       "      <td>{'Polls': 'O', 'indicate': 'O', 'that': 'O', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Local residents say the student crowds are sma...</td>\n",
       "      <td>{'Local': 'O', 'residents': 'O', 'say': 'O', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A group of Somali ministers walked out of a me...</td>\n",
       "      <td>{'A': 'O', 'group': 'O', 'of': 'O', 'Somali': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7189</th>\n",
       "      <td>Qatar had been the only Gulf Arab state to hav...</td>\n",
       "      <td>{'Qatar': 'B-geo', 'had': 'O', 'been': 'O', 't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7190</th>\n",
       "      <td>In Afghanistan , the prime minister rejected c...</td>\n",
       "      <td>{'In': 'O', 'Afghanistan': 'B-geo', ',': 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7191</th>\n",
       "      <td>The attack came after the Congolese government...</td>\n",
       "      <td>{'The': 'O', 'attack': 'O', 'came': 'O', 'afte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7192</th>\n",
       "      <td>In a separate incident today , authorities sai...</td>\n",
       "      <td>{'In': 'O', 'a': 'O', 'separate': 'O', 'incide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7193</th>\n",
       "      <td>The country is also working on iron ore and oi...</td>\n",
       "      <td>{'The': 'O', 'country': 'O', 'is': 'O', 'also'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7194 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "0     In 1784 , the French sold the island to Sweden...   \n",
       "1     Democrat Tom Daschle contradicts President Bus...   \n",
       "2     Polls indicate that if early elections are hel...   \n",
       "3     Local residents say the student crowds are sma...   \n",
       "4     A group of Somali ministers walked out of a me...   \n",
       "...                                                 ...   \n",
       "7189  Qatar had been the only Gulf Arab state to hav...   \n",
       "7190  In Afghanistan , the prime minister rejected c...   \n",
       "7191  The attack came after the Congolese government...   \n",
       "7192  In a separate incident today , authorities sai...   \n",
       "7193  The country is also working on iron ore and oi...   \n",
       "\n",
       "                                                    tag  \n",
       "0     {'In': 'O', '1784': 'B-tim', ',': 'O', 'the': ...  \n",
       "1     {'Democrat': 'O', 'Tom': 'B-per', 'Daschle': '...  \n",
       "2     {'Polls': 'O', 'indicate': 'O', 'that': 'O', '...  \n",
       "3     {'Local': 'O', 'residents': 'O', 'say': 'O', '...  \n",
       "4     {'A': 'O', 'group': 'O', 'of': 'O', 'Somali': ...  \n",
       "...                                                 ...  \n",
       "7189  {'Qatar': 'B-geo', 'had': 'O', 'been': 'O', 't...  \n",
       "7190  {'In': 'O', 'Afghanistan': 'B-geo', ',': 'O', ...  \n",
       "7191  {'The': 'O', 'attack': 'O', 'came': 'O', 'afte...  \n",
       "7192  {'In': 'O', 'a': 'O', 'separate': 'O', 'incide...  \n",
       "7193  {'The': 'O', 'country': 'O', 'is': 'O', 'also'...  \n",
       "\n",
       "[7194 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4582c14f-d481-4db0-8848-047d876558e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From 2004 to 2007 , the economy grew about 10 ...</td>\n",
       "      <td>{'From': 'B-tim', '2004': 'I-tim', 'to': 'I-ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Earlier this week , the African Union dispatch...</td>\n",
       "      <td>{'Earlier': 'O', 'this': 'O', 'week': 'O', ','...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>China 's state news agency says scientists hav...</td>\n",
       "      <td>{'China': 'B-geo', \"'s\": 'O', 'state': 'O', 'n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He said Americans are thankful for their sacri...</td>\n",
       "      <td>{'He': 'O', 'said': 'O', 'Americans': 'B-gpe',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The letter fueled charges of racism and was co...</td>\n",
       "      <td>{'The': 'O', 'letter': 'O', 'fueled': 'O', 'ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7189</th>\n",
       "      <td>Officials say money from the fines will go to ...</td>\n",
       "      <td>{'Officials': 'O', 'say': 'O', 'money': 'O', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7190</th>\n",
       "      <td>A cease-fire was reached in 1991 .</td>\n",
       "      <td>{'A': 'O', 'cease-fire': 'O', 'was': 'O', 'rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7191</th>\n",
       "      <td>He denies any involvement .</td>\n",
       "      <td>{'He': 'O', 'denies': 'O', 'any': 'O', 'involv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7192</th>\n",
       "      <td>Union head Roger Toussaint calls the dispute a...</td>\n",
       "      <td>{'Union': 'O', 'head': 'O', 'Roger': 'B-per', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7193</th>\n",
       "      <td>Christmas Eve celebrations will culminate with...</td>\n",
       "      <td>{'Christmas': 'B-eve', 'Eve': 'I-eve', 'celebr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7194 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "0     From 2004 to 2007 , the economy grew about 10 ...   \n",
       "1     Earlier this week , the African Union dispatch...   \n",
       "2     China 's state news agency says scientists hav...   \n",
       "3     He said Americans are thankful for their sacri...   \n",
       "4     The letter fueled charges of racism and was co...   \n",
       "...                                                 ...   \n",
       "7189  Officials say money from the fines will go to ...   \n",
       "7190                 A cease-fire was reached in 1991 .   \n",
       "7191                        He denies any involvement .   \n",
       "7192  Union head Roger Toussaint calls the dispute a...   \n",
       "7193  Christmas Eve celebrations will culminate with...   \n",
       "\n",
       "                                                    tag  \n",
       "0     {'From': 'B-tim', '2004': 'I-tim', 'to': 'I-ti...  \n",
       "1     {'Earlier': 'O', 'this': 'O', 'week': 'O', ','...  \n",
       "2     {'China': 'B-geo', \"'s\": 'O', 'state': 'O', 'n...  \n",
       "3     {'He': 'O', 'said': 'O', 'Americans': 'B-gpe',...  \n",
       "4     {'The': 'O', 'letter': 'O', 'fueled': 'O', 'ch...  \n",
       "...                                                 ...  \n",
       "7189  {'Officials': 'O', 'say': 'O', 'money': 'O', '...  \n",
       "7190  {'A': 'O', 'cease-fire': 'O', 'was': 'O', 'rea...  \n",
       "7191  {'He': 'O', 'denies': 'O', 'any': 'O', 'involv...  \n",
       "7192  {'Union': 'O', 'head': 'O', 'Roger': 'B-per', ...  \n",
       "7193  {'Christmas': 'B-eve', 'Eve': 'I-eve', 'celebr...  \n",
       "\n",
       "[7194 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1cd7a01-8f91-4d83-97f6-65ce53532883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (1.26.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "792bce89-b2a7-48e1-888c-f4a34bd566a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'tag'],\n",
       "        num_rows: 33571\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'tag'],\n",
       "        num_rows: 7194\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'tag'],\n",
       "        num_rows: 7194\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "# Create Datasets from DataFrames\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "valid_dataset = Dataset.from_pandas(valid_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Create DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': valid_dataset,\n",
    "    'test': test_dataset,\n",
    "})\n",
    "\n",
    "# Print dataset_dict information\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12683d6-b622-4020-8de8-155afb8d557f",
   "metadata": {},
   "source": [
    "Loading model and tokenisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f849543-4346-4899-9db5-840422031754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HF_TOKEN'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f6968be-c2d7-47ce-a274-97ed1fad4820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ed2fa4-7617-4fdc-9b48-cb35af93e5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aefcb739c7844ced98f445aa18fefa60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657c0b58810c46ed934fd5c98a5fd2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70df02f390b744099b08493e831a53ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02960640a2ff4871a0829143ccff8d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6108619d7d3149e8b067c137b18b57d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310a0c99ff274fb2a3f90edc687bca78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7028e646-8238-44a0-8d54-d29a1a517fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tf-keras) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.23.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.37.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.26.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf-keras) (0.43.0)\n",
      "Requirement already satisfied: rich in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2024.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ba19a-8553-42df-b745-e63b3126be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset_dict['test'][0]['sentence']\n",
    "label = dataset_dict['test'][0]['tag']\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ff3b651-4cd7-48a2-9f69-f2d3a06d7552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From 2004 to 2007 , the economy grew about 10 % per year , driven largely by an expansion in the garment sector , construction , agriculture , and tourism .'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de44d94a-4462-43ee-86a4-fcd84f5b7a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'From': 'B-tim', '2004': 'I-tim', 'to': 'I-tim', '2007': 'I-tim', ',': 'O', 'the': 'O', 'economy': 'O', 'grew': 'O', 'about': 'O', '10': 'O', '%': 'O', 'per': 'O', 'year': 'O', 'driven': 'O', 'largely': 'O', 'by': 'O', 'an': 'O', 'expansion': 'O', 'in': 'O', 'garment': 'O', 'sector': 'O', 'construction': 'O', 'agriculture': 'O', 'and': 'O', 'tourism': 'O', '.': 'O'}\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6367915d-fc65-47d6-9aa6-9f29ce61b848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"\\n\\nHere is the sentence : \\n'From 2004 to 2007 , the economy grew about 10 % per year , driven largely by an expansion in the garment sector , construction , agriculture , and tourism .'\\n\\nWords tags are :\\n{'From': 'B-tim', '2004': 'I-tim', 'to': 'I-tim', '2007': 'I-tim', ',': 'O', 'the': 'O', 'economy': 'O', 'grew': 'O', 'about': 'O', '10': 'O', '%': 'O', 'per': 'O', 'year': 'O', 'driven': 'O', 'largely': 'O', 'by': 'O', 'an': 'O', 'expansion': 'O', 'in': 'O', 'garment': 'O', 'sector': 'O', 'construction': 'O', 'agriculture': 'O', 'and': 'O', 'tourism': 'O', '.': 'O'}\\n\\nHere is the sentence : \\n'In Germany, life is better compared to Austria.'\\n\\nWords tags are :\\n\\n\\n{'From': 'Sueffel', '2004': 'Toilet-googler', 'to': 'Toilet-googler', '2006': 'Ribbons', 'in': 'Toilet-googler', '2007': 'Tobacco', ',': 'Diesel', '2003': 'Robos', 'in': 'toilet-googler', '2007': 'Aurora Beer', ',': 'Theatrical', '.,,': 'Tobacco', '2009': 'Spoon', '.: 'toilet-googler', '2010': 'Spoon', '.: 'toilet-googler', '.': 'toilet?'}}\\n\\nTags are :\\n\\n\\n{'From': 'Martin', '2004': 'Sieffel', '2004': 'Espresso', '2004': 'Toilet-gate', '2004': 'Spinie', '2004': 'Fries', '2005': 'Espresso', '2005': 'Jazz', '2006': 'Gingerbread', '2005': 'Lid', '2005': 'Schnappel',\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sample = \"\"\"\n",
    "\n",
    "Here is the sentence : \n",
    "'From 2004 to 2007 , the economy grew about 10 % per year , driven largely by an expansion in the garment sector , construction , agriculture , and tourism .'\n",
    "\n",
    "Words tags are :\n",
    "{'From': 'B-tim', '2004': 'I-tim', 'to': 'I-tim', '2007': 'I-tim', ',': 'O', 'the': 'O', 'economy': 'O', 'grew': 'O', 'about': 'O', '10': 'O', '%': 'O', 'per': 'O', 'year': 'O', 'driven': 'O', 'largely': 'O', 'by': 'O', 'an': 'O', 'expansion': 'O', 'in': 'O', 'garment': 'O', 'sector': 'O', 'construction': 'O', 'agriculture': 'O', 'and': 'O', 'tourism': 'O', '.': 'O'}\n",
    "\n",
    "Here is the sentence : \n",
    "'In Germany, life is better compared to Austria.'\n",
    "\n",
    "Words tags are :\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "pipe(sample, max_length=500, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0f9b9d7-88cc-4b7d-82d1-00251b139e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample\n",
      "From 2004 to 2007 , the economy grew about 10 % per year , driven largely by an expansion in the garment sector , construction , agriculture , and tourism .\n",
      "-------------------\n",
      "Model Generated Tags:\n",
      "From 2004 to 2007, the economy grew about 10 % per year, driven largely by an expansion in the garment sector, construction, agriculture, and tourism.\n",
      "-------------------\n",
      "Correct Tags:\n",
      "{'From': 'B-tim', '2004': 'I-tim', 'to': 'I-tim', '2007': 'I-tim', ',': 'O', 'the': 'O', 'economy': 'O', 'grew': 'O', 'about': 'O', '10': 'O', '%': 'O', 'per': 'O', 'year': 'O', 'driven': 'O', 'largely': 'O', 'by': 'O', 'an': 'O', 'expansion': 'O', 'in': 'O', 'garment': 'O', 'sector': 'O', 'construction': 'O', 'agriculture': 'O', 'and': 'O', 'tourism': 'O', '.': 'O'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output = generate_summary(sample, llm=model)\n",
    "print(\"Sample\")\n",
    "print(sample)\n",
    "print(\"-------------------\")\n",
    "print(\"Model Generated Tags:\")\n",
    "print(output)\n",
    "print(\"-------------------\")\n",
    "print(\"Correct Tags:\")\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30699ce8-6259-4d43-b6dc-cc7b7ec1973e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
